資料分析
---

資料分析是一個很有意思的過程，我們可以簡單地將這個過程分成四個步驟：

 - 識別需求
 - 收集資料
 - 分析資料
 - 展示資料

值得注意的是：在分析資料的過程中，需要不同的人員來參與，需要跨域多個領域的知識點——分析、設計、開發、商業和研究等領域。因此，在這樣的領域裡，迴歸敏捷也是一種不錯的選擇（源於：《敏捷資料科學》）：

 - 通才高於專長
 - 小團隊高於大團隊
 - 使用高階工具和平臺：雲端計算、分散式系統、PaaS
 - 持續、迭代地分享工作成果，即使這些工作未完成

###識別需求

在我們開始分析資料之前，我們需要明確一下，我們的問題是什麼？即，我們到底要幹嘛，我們想要的內容是什麼。

> 識別資訊需求是確保資料分析過程有效性的首要條件，可以為收集資料、分析資料提供清晰的目標。

當我們想要提到我們的網站在不同地區的速度時，就需要去探索我們的使用者主要是在哪些地區。即，現在這是我們的需求。我們已經有了這樣的一個明確的目標，下面要做起來就很輕鬆了。

###收集資料

那麼現在新的問題來了，我們的資料要從哪裡來？

對於大部分的網站來說，都會有訪問日誌。但是這些訪問日誌只能顯示某個 IP 進入了某個頁面，並不能詳細地介紹這個使用者在這個頁面待了多久，做了什麼事。這時候，就需要依賴於類似 Google Analytics 這樣的工具來統計網站的流量。還有類似於New Relic這樣的工具來統計使用者的一些行為。

在一些以科學研究為目的的資料收集中，我們可以從一些公開的資料中獲取這些資料。

而在一些特殊的情況裡，我們就需要通過爬蟲來完成這樣的工作。

###分析資料

現在，我們終於可以真正的去分析資料了——我的意思是，我們要開始寫程式碼了。從海量的資料中過濾出我們想要的資料，並通過演算法來對其進行分析。

一般來說，我們都利用現有的工具來完成大部分的工作。要使用哪一類工具，取決於我們要分析的資料的數量級了。如果只是一般的數量級，我們可以考慮用 R 語言、Python、Octave 等單機工具來完成。如果是大量的資料，那麼我們就需要考慮用 Hadoop、Spark 來完成這個級別的工作。

而一般來說，這個過程可能是要經過一系列的工具才能完成。如在之前我在分析我的部落格的日誌時(1G左右)，我用 Hadoop + Apache Pig + Jython 來將日誌中的 IP 轉換為 GEO 資訊，再將 GEO 資訊儲存到 ElasticSearch 中。隨後，我們就可以用 AMap、leaflet 這一類 GEO 庫將這些點放置到地圖上。

###展示資料

現在，終於來到我最喜歡的環節了，也是最有意思，但是卻又最難的環節。

我們過濾資料，得到想要的內容後，就要去考慮如何視覺化這些資料。在我熟悉的 Web GIS領域裡，我可以視覺化出我過濾後的那些資料。但是對於我不熟悉的領域，要視覺化這些資料不是一件容易的事。在少數情況下，可以使用現有的工具完成需求，多數情況下，我們也需要寫相當的程式碼才能將資料最後視覺化出來。

而在以什麼形式來展示我們的資料時，又是一個問題。如一般的資料結果，到底是使用柱形圖、條形圖、折線圖還是面積圖？這要求我們有一些 UX 方面的經驗。

參考來源: **精益資料分析**。
